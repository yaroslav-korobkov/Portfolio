{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Предобработка\" data-toc-modified-id=\"Предобработка-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Предобработка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Удаление-столбца-Unnamed:-0\" data-toc-modified-id=\"Удаление-столбца-Unnamed:-0-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Удаление столбца Unnamed: 0</a></span></li><li><span><a href=\"#Лемматизация,-приведение-текста-к-нижнему-регистру-и-очистка-текста\" data-toc-modified-id=\"Лемматизация,-приведение-текста-к-нижнему-регистру-и-очистка-текста-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Лемматизация, приведение текста к нижнему регистру и очистка текста</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Подготовка-признаков-и-разделение-данных-на-выборки\" data-toc-modified-id=\"Подготовка-признаков-и-разделение-данных-на-выборки-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Подготовка признаков и разделение данных на выборки</a></span></li><li><span><a href=\"#Проверка-на-пропуски\" data-toc-modified-id=\"Проверка-на-пропуски-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Проверка на пропуски</a></span></li><li><span><a href=\"#Тестирование-моделей\" data-toc-modified-id=\"Тестирование-моделей-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Тестирование моделей</a></span></li><li><span><a href=\"#Тестирование-на-тестовой-выборке\" data-toc-modified-id=\"Тестирование-на-тестовой-выборке-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Тестирование на тестовой выборке</a></span></li></ul></li><li><span><a href=\"#Общий-вывод\" data-toc-modified-id=\"Общий-вывод-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Общий вывод</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели классификации комментариев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Требуется обучить модель классифицировать комментарии на позитивные и негативные.\n",
    "\n",
    "Условия заказчика: построить модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "HTML(\"<style>.container { width:90% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345) # Фиксация псевдослучайности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = 'toxic_comments.csv'\n",
    "\n",
    "if os.path.exists(pth):\n",
    "    df = pd.read_csv(pth)\n",
    "else:\n",
    "    cloud_url = ''\n",
    "    try:\n",
    "        df = pd.read_csv(cloud_url)\n",
    "    except:\n",
    "        print(\"Failed to load data from the cloud.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** Из первого взгляда на данные можно сказать\n",
    "- Нужно привести текст к нижнему регистру, то поможет снизить размерность данных и сделает слова с разным регистром однородными\n",
    "- Требуется удалить лишние символы: можно удалить пунктуацию, специальные символы и цифры, так как они могут не нести смысловую нагрузку\n",
    "- Нужно удалить столбец Unnamed: 0, так как он просто копирует индексы и не несет информационной нагрузки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление столбца Unnamed: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns='Unnamed: 0')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация, приведение текста к нижнему регистру и очистка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatize(texts):\n",
    "    print(\"1. Входные тексты:\", texts[:2])\n",
    "    \n",
    "    # Приведение текстов к нижнему регистру\n",
    "    texts_lower = [text.lower() for text in texts]\n",
    "    print(\"2. Тексты в нижнем регистре:\", texts_lower[:2])\n",
    "    \n",
    "    # Удаление всех символов, кроме английских букв и пробелов, с помощью регулярных выражений\n",
    "    texts_cleaned = [re.sub(r'[^a-z\\']', ' ', text) for text in texts_lower]\n",
    "    print(\"3. Очищенные тексты:\", texts_cleaned[:2])\n",
    "    \n",
    "    # Разделение слов пробелами, чтобы получить чистые слова\n",
    "    texts_cleaned_split = [\" \".join(text.split()) for text in texts_cleaned]\n",
    "    print(\"4. Разделенные слова:\", texts_cleaned_split[:2])\n",
    "    \n",
    "    # Лемматизация текстов с помощью Spacy в пакетном режиме\n",
    "    docs = list(nlp.pipe(texts_cleaned_split, batch_size=1000))\n",
    "    lemmatized_texts = [' '.join([token.lemma_ for token in doc]) for doc in docs]\n",
    "    print(\"5. Лемматизированные тексты:\", lemmatized_texts[:2])\n",
    "    \n",
    "    return lemmatized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно загружены из файла.\n",
      "Очищенный и лемматизированный текст: explanation why the edit make under my username hardcore metallica fan be revert they be not vandalism just closure on some gas after I vote at new york dolls fac and please do not remove the template from the talk page since I be retire now\n"
     ]
    }
   ],
   "source": [
    "# Попробуем загрузить данные из файла, если файл существует\n",
    "try:\n",
    "    with open(\"lemmatized_texts.pkl\", \"rb\") as file:\n",
    "        corpus = pickle.load(file)\n",
    "    print(\"Данные успешно загружены из файла.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    # Если файл не найден, выполняем лемматизацию\n",
    "    corpus = df['text'].values\n",
    "    corpus = lemmatize(corpus)\n",
    "    \n",
    "    # Записываем результаты лемматизации в файл\n",
    "    with open(\"lemmatized_texts.pkl\", \"wb\") as file:\n",
    "        pickle.dump(corpus, file)\n",
    "    print(\"Результаты лемматизации сохранены в файл.\")\n",
    "\n",
    "print('Очищенный и лемматизированный текст:', corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** в ходе предобработки данных\n",
    "- Удален столбец Unnamed\n",
    "- Проведена лемматизация, а также приведение к нижнему регистру и очистка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка признаков и разделение данных на выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data percentage: 75.0%\n",
      "Test data percentage: 25.0%\n"
     ]
    }
   ],
   "source": [
    "# Преобразовываем целевой признак в массив\n",
    "y = df['toxic'].values\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, stratify=y)\n",
    "\n",
    "train_percentage = len(X_train) / len(corpus) * 100\n",
    "test_percentage = len(X_test) / len(corpus) * 100\n",
    "\n",
    "print(f\"Train data percentage: {train_percentage}%\")\n",
    "print(f\"Test data percentage: {test_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yarma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk_stopwords.words('english')\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка на пропуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропуски в X_train_tfidf: False\n",
      "Пропуски в X_test_tfidf: False\n",
      "Нули в X_train_tfidf: False\n",
      "Нули в X_test_tfidf: False\n"
     ]
    }
   ],
   "source": [
    "# Проверяем наличие пропусков в X_train_tfidf\n",
    "has_nan_train = np.isnan(X_train_tfidf.data).any()\n",
    "print(f\"Пропуски в X_train_tfidf: {has_nan_train}\")\n",
    "\n",
    "# Проверяем наличие пропусков в X_test_tfidf\n",
    "has_nan_test = np.isnan(X_test_tfidf.data).any()\n",
    "print(f\"Пропуски в X_test_tfidf: {has_nan_test}\")\n",
    "\n",
    "# Проверяем наличие нулевых элементов в X_train_tfidf\n",
    "has_zeros_train = (X_train_tfidf.data == 0).any()\n",
    "print(f\"Нули в X_train_tfidf: {has_zeros_train}\")\n",
    "\n",
    "# Проверяем наличие нулевых элементов в X_test_tfidf\n",
    "has_zeros_test = (X_test_tfidf.data == 0).any()\n",
    "print(f\"Нули в X_test_tfidf: {has_zeros_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Привести признаки TF-IDF к типу np.float32 для экономии памяти \n",
    "X_train_tfidf = X_train_tfidf.astype(np.float32)\n",
    "X_test_tfidf = X_test_tfidf.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование моделей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4962580\ttotal: 569ms\tremaining: 9m 28s\n",
      "250:\tlearn: 0.7368579\ttotal: 1m 56s\tremaining: 5m 47s\n",
      "500:\tlearn: 0.7778691\ttotal: 3m 53s\tremaining: 3m 52s\n",
      "750:\tlearn: 0.7968728\ttotal: 5m 45s\tremaining: 1m 54s\n",
      "999:\tlearn: 0.8124352\ttotal: 7m 33s\tremaining: 0us\n",
      "Лучшие параметры для модели CatBoostClassifier:\n",
      "{'depth': 6, 'eval_metric': 'F1', 'learning_rate': 0.1, 'verbose': 250}\n",
      "Лучший F1-скор для модели CatBoostClassifier:\n",
      "0.7532364592726895\n",
      "F1-скор соответствует требованиям. Качество модели хорошее.\n",
      "\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 12140, number of negative: 107329\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.687420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 515117\n",
      "[LightGBM] [Info] Number of data points in the train set: 119469, number of used features: 9429\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101616 -> initscore=-2.179393\n",
      "[LightGBM] [Info] Start training from score -2.179393\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Лучшие параметры для модели LGBMClassifier:\n",
      "{'learning_rate': 0.1, 'max_depth': 6, 'metric': 'binary_logloss', 'num_leaves': 31, 'objective': 'binary'}\n",
      "Лучший F1-скор для модели LGBMClassifier:\n",
      "0.6654255338010987\n",
      "F1-скор требует улучшения для данной модели.\n",
      "\n",
      "Лучшие параметры для модели LogisticRegression:\n",
      "{'C': 10, 'max_iter': 1000}\n",
      "Лучший F1-скор для модели LogisticRegression:\n",
      "0.7646883545618889\n",
      "F1-скор соответствует требованиям. Качество модели хорошее.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Определим различные модели для классификации\n",
    "models = {\n",
    "    'CatBoostClassifier': CatBoostClassifier(),\n",
    "    'LGBMClassifier': LGBMClassifier(),\n",
    "    'LogisticRegression': LogisticRegression()\n",
    "}\n",
    "\n",
    "# Настроим сетки гиперпараметров для каждой модели\n",
    "param_grid_catboost = {\n",
    "    'learning_rate': [0.1],\n",
    "    'depth': [6],\n",
    "    'verbose': [250],\n",
    "    'eval_metric': ['F1']\n",
    "}\n",
    "\n",
    "param_grid_lightgbm = {\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [6],\n",
    "    'num_leaves': [31],\n",
    "    'objective': ['binary'],\n",
    "    'metric': ['binary_logloss']\n",
    "}\n",
    "\n",
    "param_grid_logistic_regression = {\n",
    "    'C': [1, 10, 100],\n",
    "    'max_iter' : [1000]\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'CatBoostClassifier': param_grid_catboost,\n",
    "    'LGBMClassifier': param_grid_lightgbm,\n",
    "    'LogisticRegression': param_grid_logistic_regression\n",
    "}\n",
    "\n",
    "# Используем стратифицированную кросс-валидацию\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "best_params_dict = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    param_grid_model = param_grids[model_name]\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid_model, cv=cv, scoring='f1', error_score='raise', n_jobs=-1)\n",
    "    grid_search.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    best_params_dict[model_name] = grid_search.best_params_\n",
    "\n",
    "    print(f\"Лучшие параметры для модели {model_name}:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    best_f1 = grid_search.best_score_\n",
    "    \n",
    "    print(f\"Лучший F1-скор для модели {model_name}:\")\n",
    "    print(best_f1)\n",
    "    \n",
    "    if best_f1 > 0.75:  \n",
    "        print(\"F1-скор соответствует требованиям. Качество модели хорошее.\")\n",
    "    else:\n",
    "        print(\"F1-скор требует улучшения для данной модели.\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** в ходе данного этапа проекта\n",
    "- Подготовлены признаки \n",
    "- Данные разделены на выборки \n",
    "- Проведена проверка на пропуски и нулевые значения в тренировочной и тестовой выборках\n",
    "- В ходе обучения моделей лучший результат показала LogisticRegression (F1~0.76), протестируем ее на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование на тестовой выборке "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7787099436116077\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(**best_params_dict['LogisticRegression'])\n",
    "\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "predict = model.predict(X_test_tfidf)\n",
    "\n",
    "print(f1_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Комментарий:**\n",
    "F1 LogisticRegression на тестовой выборке составила ~0.78, что соответствует требованиям закачника"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проект \"Классификация комментариев в интернет-магазине\" был успешно выполнен с целью построения модели, способной классифицировать комментарии на позитивные и негативные. Этот инструмент необходим для нового сервиса интернет-магазина \"Викишоп\", который позволяет пользователям редактировать описания товаров, как в вики-сообществах.\n",
    "\n",
    "В ходе проекта были выполнены следующие ключевые шаги:\n",
    "\n",
    "    Предобработка данных: текст комментариев был приведен к нижнему регистру, произведена лемматизация, а также удалены лишние символы, такие как пунктуация, специальные символы и цифры. Эти шаги помогли снизить размерность данных и сделали текст более однородным.\n",
    "\n",
    "    Подготовка признаков: тексты комментариев были преобразованы в числовые векторы с использованием метода TF-IDF.\n",
    "\n",
    "    Обучение моделей: были опробованы различные алгоритмы машинного обучения, такие как CatBoost, LightGBM и Logistic Regression. Лучший результат показала модель Logistic Regression с F1-скором около 0.76.\n",
    "\n",
    "    Оценка результатов: выбранная модель была протестирована на тестовой выборке, и F1-скор составил около 0.78, что соответствует требованиям заказчика.\n",
    "\n",
    "    Заключение: Проект успешно достиг цели построения модели для классификации комментариев. Разработанный инструмент поможет интернет-магазину \"Викишоп\" автоматизировать процесс модерации комментариев, обеспечивая безопасное и комфортное взаимодействие пользователей с описаниями товаров.\n",
    "\n",
    "В целом, успешное завершение проекта позволит интернет-магазину \"Викишоп\" обеспечить более безопасное и качественное взаимодействие пользователей с комментариями, повысив уровень доверия и удовлетворенности клиентов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "216px",
    "width": "666px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
